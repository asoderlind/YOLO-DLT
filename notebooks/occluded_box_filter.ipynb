{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ccebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def xywh2xyxy(x_center, y_center, w_box, h_box, width, height):\n",
    "    \"\"\"\n",
    "    Convert YOLO format (x_center, y_center, w_box, h_box) to (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    x1 = int((x_center - w_box / 2) * width)\n",
    "    y1 = int((y_center - h_box / 2) * height)\n",
    "    x2 = int((x_center + w_box / 2) * width)\n",
    "    y2 = int((y_center + h_box / 2) * height)\n",
    "\n",
    "    # Clamp the coordinates to be within the image dimensions\n",
    "    x1 = max(0, min(width - 1, x1))\n",
    "    y1 = max(0, min(height - 1, y1))\n",
    "    x2 = max(0, min(width - 1, x2))\n",
    "    y2 = max(0, min(height - 1, y2))\n",
    "\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "def filter_occluded_boxes(label_file, img_height, img_width, occlusion_threshold=0.5):\n",
    "    # Make annotations array\n",
    "    with open(label_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    num_lines = len(lines)\n",
    "    annotations = np.zeros((num_lines, 5), dtype=np.float32)\n",
    "    for i, line in enumerate(lines):\n",
    "        # Split the line into components\n",
    "        components = line.strip().split()\n",
    "        # Extract the bounding box coordinates\n",
    "        x_center, y_center, w_box, h_box, dist = map(float, components[1:6])\n",
    "\n",
    "        # Convert to (x1, y1, x2, y2) format\n",
    "        x1, y1, x2, y2 = xywh2xyxy(x_center, y_center, w_box, h_box, img_width, img_height)\n",
    "\n",
    "        # Append the bounding box to the list\n",
    "        annotations[i, 0] = x1\n",
    "        annotations[i, 1] = y1\n",
    "        annotations[i, 2] = x2\n",
    "        annotations[i, 3] = y2\n",
    "        annotations[i, 4] = dist\n",
    "\n",
    "\n",
    "    # Sort the annotations by distance, largest to smallest\n",
    "    annotations = annotations[np.argsort(annotations[:, 4])[::-1]]\n",
    "\n",
    "    # Create a mock image with the same height and width as the image\n",
    "    # and a channel for classification\n",
    "    mock_image = np.zeros((img_height, img_width), dtype=np.int8)\n",
    "    mock_image.fill(-1)\n",
    "\n",
    "    total_area_per_annotation = np.zeros(num_lines, dtype=np.float32)\n",
    "    visible_area_per_annotation = np.zeros(num_lines, dtype=np.float32)\n",
    "\n",
    "    # Iterate over each annotation\n",
    "    for i in range(num_lines):\n",
    "        # Get the coordinates of the bounding box\n",
    "        x1, y1, x2, y2 = annotations[i, :4].astype(int)\n",
    "\n",
    "        # Calculate the area of the bounding box\n",
    "        total_area = (x2 - x1) * (y2 - y1)\n",
    "        total_area_per_annotation[i] = total_area\n",
    "\n",
    "        # Add the mask to the mock image\n",
    "        mock_image[y1:y2, x1:x2] = i\n",
    "\n",
    "    for i in range(num_lines):\n",
    "        # Compare the total area with the visible area\n",
    "        visible_area = np.sum(mock_image == i)\n",
    "        visible_area_per_annotation[i] = visible_area\n",
    "\n",
    "    visibility_ratio_per_annotation = visible_area_per_annotation / total_area_per_annotation\n",
    "\n",
    "    # Filter out annotations with ratio < 0.5\n",
    "    valid_indices = np.where(visibility_ratio_per_annotation >= occlusion_threshold)[0]\n",
    "\n",
    "    annotations = annotations[valid_indices]\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581cea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the bounding boxes on top of a blank image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_label = \"../../yolo-testing/datasets/carla-world06-day/labels/val/vid_86958-TOwn06-150npc_frame_000065.txt\"\n",
    "img_height, img_width = 600, 800  # Set the desired height and width\n",
    "annotations = filter_occluded_boxes(sample_label, img_height, img_width, occlusion_threshold=0.5)\n",
    "\n",
    "# Create a blank image\n",
    "h, w = 600, 800  # Set the desired height and width\n",
    "image = np.zeros((h,w, 3), dtype=np.uint8)\n",
    "# Set the color for the bounding boxes (BGR format)\n",
    "color = (0, 255, 0)  # Green\n",
    "# Set the thickness of the bounding box lines\n",
    "thickness = 2\n",
    "# Draw the bounding boxes on the image\n",
    "for i in range(annotations.shape[0]):\n",
    "    bbox = annotations[i, :4]\n",
    "    x1, y1, x2, y2 = map(int, bbox)\n",
    "    # Draw the rectangle on the image\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)\n",
    "# Display the image with bounding boxes\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")  # Hide the axes\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
