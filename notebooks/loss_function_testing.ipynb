{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schemas\n",
    "\n",
    "## V8DetectionLoss - call function\n",
    "\n",
    "```python\n",
    "# 145 = 80 + 4 * 16 + 1\n",
    "feats = num_classes + box_coord * dfl_channels + distance\n",
    "```\n",
    "\n",
    "**Training**\n",
    "```python\n",
    "preds = list[\n",
    "    tensor[1, 145, 80, 80], # [bs, feats, h/1, w/1]\n",
    "    tensor[1, 145, 40, 40], # [bs, feats, h/1, w/1]\n",
    "    tensor[1, 145, 20, 20]  # [bs, feats, h/1, w/1]\n",
    "    ]\n",
    "```\n",
    "\n",
    "**Validation**\n",
    "```python\n",
    "preds = tuple[\n",
    "    tensor[2, 84, 6174], # [bs, ??, ??] last value depends on input image\n",
    "    list[\n",
    "        tensor[2, 145, 56, 84], # [bs, feats, h/1, w/1] height and width depends on input image\n",
    "        tensor[2, 145, 28, 42], # [bs, feats, h/2, w/2]\n",
    "        tensor[2, 145, 14, 21]  # [bs, feats, h/4, w/4]\n",
    "    ]\n",
    "\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds.shape [torch.Size([1, 145, 80, 80]), torch.Size([1, 145, 40, 40]), torch.Size([1, 145, 20, 20])]\n",
      "pred_scores.shape torch.Size([1, 8400, 80])\n",
      "pred_distri.shape torch.Size([1, 8400, 64])\n",
      "pred_dist.shape torch.Size([1, 8400, 1])\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics.utils.tal import make_anchors\n",
    "no = 145\n",
    "use_dist = True\n",
    "device = torch.device('mps')\n",
    "stride = torch.tensor([8, 16, 32], device=device)\n",
    "\n",
    "og_w = 640\n",
    "og_h = 640\n",
    "\n",
    "input_w = og_w//8\n",
    "input_h = og_h//8\n",
    "\n",
    "preds = [\n",
    "    torch.randn(1, no, input_h, input_w),\n",
    "    torch.randn(1, no, input_h//2, input_w//2),\n",
    "    torch.randn(1, no, input_h//4, input_w//4),\n",
    "]\n",
    "feats = preds\n",
    "print(\"preds.shape\", [p.shape for p in preds])\n",
    "\n",
    "#print(\"preds.shape\", [p.shape for p in preds])\n",
    "#print(\"---\")\n",
    "\n",
    "preds = [xi.view(preds[0].shape[0], no, -1) for xi in preds]\n",
    "#print(\"preds.shape\", [p.shape for p in preds])\n",
    "#print(\"---\")\n",
    "preds = torch.cat(preds, 2)\n",
    "#print(\"preds.shape\", preds.shape)\n",
    "#print(\"---\")\n",
    "pred_distri, pred_scores, pred_dist = torch.split(preds, [64, 80, 1], dim=1)\n",
    "#print(\"pred_distri.shape\", pred_distri.shape)\n",
    "#print(\"pred_scores.shape\", pred_scores.shape)\n",
    "#print(\"pred_dist.shape\", pred_dist.shape) # [1, 1, 8400] 8400 = 80*80 + 40*40 + 20*20\n",
    "#print(\"---\")\n",
    "\n",
    "pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n",
    "pred_distri = pred_distri.permute(0, 2, 1).contiguous()\n",
    "pred_dist = pred_dist.permute(0, 2, 1).contiguous()\n",
    "print(\"pred_scores.shape\", pred_scores.shape) # [1, 8400, 80]\n",
    "print(\"pred_distri.shape\", pred_distri.shape) # [1, 8400, (4*16)]\n",
    "print(\"pred_dist.shape\", pred_dist.shape) # [1, 8400, 1]\n",
    "print(\"---\")\n",
    "dtype = pred_scores.dtype\n",
    "batch_size = pred_scores.shape[0]\n",
    "imgsz = torch.tensor(feats[0].shape[2:], device=device, dtype=dtype) * stride[0]  # image size (h,w)\n",
    "anchor_points, stride_tensor = make_anchors(feats, stride, 0.5)\n",
    "#print(\"dtype\", dtype) # torch.float32\n",
    "#print(\"batch_size\", batch_size) # 1\n",
    "#print(\"imgsz\", imgsz) # [640, 640]\n",
    "#print(\"anchor_points.shape\", anchor_points.shape) # [8400, 2]\n",
    "#print(\"stride_tensor.shape\", stride_tensor.shape) # [8400, 1]\n",
    "#print(\"---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_tensor.shape torch.Size([1, 8400, 1])\n",
      "tensor(0.7000)\n",
      "tensor(0.7000)\n",
      "tensor(0.7000)\n",
      "tensor(0.7000)\n",
      "tensor(0.8000)\n",
      "tensor(0.6000)\n",
      "tensor(0.6000)\n",
      "tensor(0.6000)\n",
      "tensor(0.6000)\n"
     ]
    }
   ],
   "source": [
    "# 5 annotated objects with distances\n",
    "gt_distances = torch.zeros(1, 5, 1)\n",
    "gt_distances[0, 0, 0] = 0.8\n",
    "gt_distances[0, 1, 0] = 0.7\n",
    "gt_distances[0, 2, 0] = 0.6\n",
    "gt_distances[0, 3, 0] = 0.5\n",
    "gt_distances[0, 4, 0] = 0.4\n",
    "\n",
    "target_gt_idx = torch.zeros(1,8400).long()\n",
    "target_gt_idx[0, 0] = 1\n",
    "target_gt_idx[0, 1] = 1\n",
    "target_gt_idx[0, 2] = 1\n",
    "target_gt_idx[0, 3] = 1\n",
    "\n",
    "target_gt_idx[0, 8396] = 2\n",
    "target_gt_idx[0, 8397] = 2\n",
    "target_gt_idx[0, 8398] = 2\n",
    "target_gt_idx[0, 8399] = 2\n",
    "\n",
    "new_tensor = gt_distances.view(-1, gt_distances.shape[-1])[target_gt_idx]\n",
    "print(\"new_tensor.shape\", new_tensor.shape)\n",
    "print(new_tensor[0][0][0])\n",
    "print(new_tensor[0][1][0])\n",
    "print(new_tensor[0][2][0])\n",
    "print(new_tensor[0][3][0])\n",
    "print(new_tensor[0][4][0])\n",
    "print(new_tensor[0][8396][0])\n",
    "print(new_tensor[0][8397][0])\n",
    "print(new_tensor[0][8398][0])\n",
    "print(new_tensor[0][8399][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
